{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "qp9Nbc1UuGY2",
      "metadata": {
        "id": "qp9Nbc1UuGY2"
      },
      "source": [
        "# NLP Text Preprocessing Notebook\n",
        "This notebook covers:\n",
        "- Text Cleaning\n",
        "- Tokenization\n",
        "- Stopwords\n",
        "- Stemming & Lemmatization\n",
        "- N-grams\n",
        "- Basic Frequency Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ObzX0t-kuGY6",
      "metadata": {
        "id": "ObzX0t-kuGY6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "gH9ZiCC1uGY9",
      "metadata": {
        "id": "gH9ZiCC1uGY9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'hello this is nlp class  learn text preprocessing google meet'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"Hello!!! This is NLP Class 2025. Learn Text Preprocessing @Google Meet. :)\"\n",
        "cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "cleaned_text = cleaned_text.lower().strip()\n",
        "cleaned_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "cbdd4882",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ow74PXhVuGY-",
      "metadata": {
        "id": "ow74PXhVuGY-"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['hello',\n",
              " 'this',\n",
              " 'is',\n",
              " 'nlp',\n",
              " 'class',\n",
              " 'learn',\n",
              " 'text',\n",
              " 'preprocessing',\n",
              " 'google',\n",
              " 'meet']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "tokens = word_tokenize(cleaned_text)\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "HPI2MY5guGY-",
      "metadata": {
        "id": "HPI2MY5guGY-"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['hello', 'nlp', 'class', 'learn', 'text', 'preprocessing', 'google', 'meet']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "filtered_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "tLqy5qMluGY-",
      "metadata": {
        "id": "tLqy5qMluGY-"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['hello', 'nlp', 'class', 'learn', 'text', 'preprocess', 'googl', 'meet']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ps = PorterStemmer()\n",
        "stemmed = [ps.stem(word) for word in filtered_tokens]\n",
        "stemmed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "GdR46CiiuGY_",
      "metadata": {
        "id": "GdR46CiiuGY_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['hello', 'nlp', 'class', 'learn', 'text', 'preprocessing', 'google', 'meet']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "lemmatized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "48z-U-3AuGY_",
      "metadata": {
        "id": "48z-U-3AuGY_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([('hello', 'this'),\n",
              "  ('this', 'is'),\n",
              "  ('is', 'nlp'),\n",
              "  ('nlp', 'class'),\n",
              "  ('class', 'learn'),\n",
              "  ('learn', 'text'),\n",
              "  ('text', 'preprocessing'),\n",
              "  ('preprocessing', 'google'),\n",
              "  ('google', 'meet')],\n",
              " [('hello', 'this', 'is'),\n",
              "  ('this', 'is', 'nlp'),\n",
              "  ('is', 'nlp', 'class'),\n",
              "  ('nlp', 'class', 'learn'),\n",
              "  ('class', 'learn', 'text'),\n",
              "  ('learn', 'text', 'preprocessing'),\n",
              "  ('text', 'preprocessing', 'google'),\n",
              "  ('preprocessing', 'google', 'meet')])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bigrams = list(ngrams(tokens, 2))\n",
        "trigrams = list(ngrams(tokens, 3))\n",
        "bigrams, trigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ShlR6D7SuGZA",
      "metadata": {
        "id": "ShlR6D7SuGZA"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter({'hello': 1,\n",
              "         'this': 1,\n",
              "         'is': 1,\n",
              "         'nlp': 1,\n",
              "         'class': 1,\n",
              "         'learn': 1,\n",
              "         'text': 1,\n",
              "         'preprocessing': 1,\n",
              "         'google': 1,\n",
              "         'meet': 1})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Counter(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "5JtUvI0hkIVm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JtUvI0hkIVm",
        "outputId": "000c371b-8e78-40a4-bed8-528886278fd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in c:\\users\\user\\anaconda3\\envs\\tf\\lib\\site-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\user\\anaconda3\\envs\\tf\\lib\\site-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\user\\anaconda3\\envs\\tf\\lib\\site-packages (from gensim) (1.16.2)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in c:\\users\\user\\anaconda3\\envs\\tf\\lib\\site-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in c:\\users\\user\\anaconda3\\envs\\tf\\lib\\site-packages (from smart_open>=1.8.1->gensim) (1.17.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9wmGiMJ6j8tD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "9wmGiMJ6j8tD",
        "outputId": "30a7634c-d6d5-4bf1-fcfa-2308d2f34e41"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I love this product, it is amazing and works g...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Terrible service, I will never buy again</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Really enjoyed the experience, very satisfied</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The product broke within days, worst purchase ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Delicious food and quick delivery</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Not worth the money, extremely disappointed</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label\n",
              "0  I love this product, it is amazing and works g...      1\n",
              "1           Terrible service, I will never buy again      0\n",
              "2      Really enjoyed the experience, very satisfied      1\n",
              "3  The product broke within days, worst purchase ...      0\n",
              "4                  Delicious food and quick delivery      1\n",
              "5        Not worth the money, extremely disappointed      0"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Imports & sample data =====\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, roc_auc_score, confusion_matrix\n",
        "\n",
        "# Optional for Word2Vec\n",
        "# pip install gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Optional for sentence embeddings\n",
        "# pip install sentence-transformers\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# For saving\n",
        "import joblib\n",
        "\n",
        "# Sample small dataset (binary sentiment)\n",
        "data = {\n",
        "    \"text\": [\n",
        "        \"I love this product, it is amazing and works great\",\n",
        "        \"Terrible service, I will never buy again\",\n",
        "        \"Really enjoyed the experience, very satisfied\",\n",
        "        \"The product broke within days, worst purchase ever\",\n",
        "        \"Delicious food and quick delivery\",\n",
        "        \"Not worth the money, extremely disappointed\"\n",
        "    ],\n",
        "    \"label\": [1, 0, 1, 0, 1, 0]  # 1 -> positive, 0 -> negative\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "V4Vr6D87kO7J",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4Vr6D87kO7J",
        "outputId": "e2157cc1-0b8f-4b0c-8e3d-86049c0e083f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BoW features shape: (6, 36)\n",
            "BoW feature names: ['again' 'amazing' 'and' 'broke' 'buy' 'days' 'delicious' 'delivery'\n",
            " 'disappointed' 'enjoyed' 'ever' 'experience' 'extremely' 'food' 'great'\n",
            " 'is' 'it' 'love' 'money' 'never' 'not' 'product' 'purchase' 'quick'\n",
            " 'really' 'satisfied' 'service' 'terrible' 'the' 'this' 'very' 'will'\n",
            " 'within' 'works' 'worst' 'worth']\n",
            "TF-IDF features shape: (6, 70)\n",
            "Example TF-IDF feature names (first 20): ['again' 'amazing' 'amazing and' 'and' 'and quick' 'and works' 'broke'\n",
            " 'broke within' 'buy' 'buy again' 'days' 'days worst' 'delicious'\n",
            " 'delicious food' 'delivery' 'disappointed' 'enjoyed' 'enjoyed the' 'ever'\n",
            " 'experience']\n"
          ]
        }
      ],
      "source": [
        "# BoW and TF-IDF examples =====\n",
        "texts = df['text'].tolist()\n",
        "\n",
        "# Bag of Words\n",
        "cv = CountVectorizer(ngram_range=(1,1), min_df=1)\n",
        "X_bow = cv.fit_transform(texts)\n",
        "print(\"BoW features shape:\", X_bow.shape)\n",
        "print(\"BoW feature names:\", cv.get_feature_names_out())\n",
        "\n",
        "# TF-IDF\n",
        "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=1)  # unigrams + bigrams\n",
        "X_tfidf = tfidf.fit_transform(texts)\n",
        "print(\"TF-IDF features shape:\", X_tfidf.shape)\n",
        "print(\"Example TF-IDF feature names (first 20):\", tfidf.get_feature_names_out()[:20])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "VtzBUlu6kW-u",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtzBUlu6kW-u",
        "outputId": "74350d3c-2016-4a1a-9739-8ad594385019"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector for 'product' (shape): (50,)\n",
            "Sentence embeddings shape: (6, 50)\n"
          ]
        }
      ],
      "source": [
        "# Word2Vec demo =====\n",
        "# Tokenize sentences simply (after your preprocessing pipeline)\n",
        "tokenized = [t.lower().split() for t in texts]\n",
        "w2v_model = Word2Vec(sentences=tokenized, vector_size=50, window=5, min_count=1, workers=1, seed=42)\n",
        "\n",
        "# Get embedding for a word\n",
        "print(\"Vector for 'product' (shape):\", w2v_model.wv['product'].shape)\n",
        "\n",
        "# To get sentence embedding: average word vectors (simple)\n",
        "def sentence_vector(sentence, model):\n",
        "    toks = sentence.lower().split()\n",
        "    vecs = [model.wv[w] for w in toks if w in model.wv]\n",
        "    if len(vecs)==0:\n",
        "        return np.zeros(model.vector_size)\n",
        "    return np.mean(vecs, axis=0)\n",
        "\n",
        "sent_emb = np.vstack([sentence_vector(s, w2v_model) for s in texts])\n",
        "print(\"Sentence embeddings shape:\", sent_emb.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "vLXSKzjNkau2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLXSKzjNkau2",
        "outputId": "de5616ad-cdd3-4cda-8fd6-12e6b3329143"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: (4, 70) Test: (2, 70)\n"
          ]
        }
      ],
      "source": [
        "# Train-test split (use TF-IDF features) =====\n",
        "X = X_tfidf\n",
        "y = df['label'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)\n",
        "print(\"Train:\", X_train.shape, \"Test:\", X_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "RUTRGC40kbc1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUTRGC40kbc1",
        "outputId": "2c74b3cc-e696-4b84-8b5d-b46b9f7e1f55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NaiveBayes -> Accuracy: 0.500\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         1\n",
            "           1       0.50      1.00      0.67         1\n",
            "\n",
            "    accuracy                           0.50         2\n",
            "   macro avg       0.25      0.50      0.33         2\n",
            "weighted avg       0.25      0.50      0.33         2\n",
            "\n",
            "LogisticRegression -> Accuracy: 0.500\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         1\n",
            "           1       0.50      1.00      0.67         1\n",
            "\n",
            "    accuracy                           0.50         2\n",
            "   macro avg       0.25      0.50      0.33         2\n",
            "weighted avg       0.25      0.50      0.33         2\n",
            "\n",
            "LinearSVC -> Accuracy: 0.500\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         1\n",
            "           1       0.50      1.00      0.67         1\n",
            "\n",
            "    accuracy                           0.50         2\n",
            "   macro avg       0.25      0.50      0.33         2\n",
            "weighted avg       0.25      0.50      0.33         2\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\anaconda3\\envs\\tf\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\User\\anaconda3\\envs\\tf\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\User\\anaconda3\\envs\\tf\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\User\\anaconda3\\envs\\tf\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\User\\anaconda3\\envs\\tf\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\User\\anaconda3\\envs\\tf\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\User\\anaconda3\\envs\\tf\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\User\\anaconda3\\envs\\tf\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\User\\anaconda3\\envs\\tf\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\User\\anaconda3\\envs\\tf\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# Train baseline models =====\n",
        "models = {\n",
        "    \"NaiveBayes\": MultinomialNB(),\n",
        "    \"LogisticRegression\": LogisticRegression(max_iter=1000, solver='liblinear'),\n",
        "    \"LinearSVC\": LinearSVC(max_iter=10000)\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, preds)\n",
        "    print(f\"{name} -> Accuracy: {acc:.3f}\")\n",
        "    print(classification_report(y_test, preds))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "5rmPwV6gkeZC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rmPwV6gkeZC",
        "outputId": "0e233d69-0d0a-4c3e-f7b5-5ff96b6fa1a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.5\n",
            "Precision: 0.5 Recall: 1.0 F1: 0.6666666666666666\n",
            "ROC-AUC: 1.0\n",
            "Confusion Matrix:\n",
            " [[0 1]\n",
            " [0 1]]\n",
            "---- Detailed report ----\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         1\n",
            "           1       0.50      1.00      0.67         1\n",
            "\n",
            "    accuracy                           0.50         2\n",
            "   macro avg       0.25      0.50      0.33         2\n",
            "weighted avg       0.25      0.50      0.33         2\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\anaconda3\\envs\\tf\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\User\\anaconda3\\envs\\tf\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\User\\anaconda3\\envs\\tf\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# Evaluation utilities =====\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        probs = model.predict_proba(X_test)[:,1]\n",
        "    else:\n",
        "        # fallback: use decision_function if available and scale to [0,1]\n",
        "        if hasattr(model, \"decision_function\"):\n",
        "            from sklearn.preprocessing import MinMaxScaler\n",
        "            scores = model.decision_function(X_test).reshape(-1,1)\n",
        "            probs = MinMaxScaler().fit_transform(scores).ravel()\n",
        "        else:\n",
        "            probs = None\n",
        "\n",
        "    preds = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, preds)\n",
        "    p, r, f1, _ = precision_recall_fscore_support(y_test, preds, average='binary', zero_division=0)\n",
        "    print(\"Accuracy:\", acc)\n",
        "    print(\"Precision:\", p, \"Recall:\", r, \"F1:\", f1)\n",
        "    if probs is not None:\n",
        "        try:\n",
        "            print(\"ROC-AUC:\", roc_auc_score(y_test, probs))\n",
        "        except:\n",
        "            pass\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, preds))\n",
        "    print(\"---- Detailed report ----\")\n",
        "    print(classification_report(y_test, preds))\n",
        "\n",
        "# Example run on the logistic regression model\n",
        "evaluate_model(models['LogisticRegression'], X_test, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "qtoVvaSZki93",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtoVvaSZki93",
        "outputId": "859958d3-dc30-4981-9d96-093291149574"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params: {'clf__C': 0.1, 'tfidf__min_df': 1, 'tfidf__ngram_range': (1, 1)}\n",
            "Best CV score: 0.4444444444444444\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         1\n",
            "           1       0.50      1.00      0.67         1\n",
            "\n",
            "    accuracy                           0.50         2\n",
            "   macro avg       0.25      0.50      0.33         2\n",
            "weighted avg       0.25      0.50      0.33         2\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\anaconda3\\envs\\tf\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\User\\anaconda3\\envs\\tf\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\User\\anaconda3\\envs\\tf\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# Pipeline + GridSearch =====\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('clf', LogisticRegression(solver='liblinear', max_iter=1000))\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    'tfidf__ngram_range': [(1,1), (1,2)],\n",
        "    'tfidf__min_df': [1, 2],\n",
        "    'clf__C': [0.1, 1, 10]\n",
        "}\n",
        "\n",
        "gs = GridSearchCV(pipeline, param_grid, cv=3, scoring='f1', n_jobs=-1)\n",
        "gs.fit(df['text'], df['label'])\n",
        "print(\"Best params:\", gs.best_params_)\n",
        "print(\"Best CV score:\", gs.best_score_)\n",
        "\n",
        "# Evaluate best model on holdout (we'll split again)\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(df['text'], df['label'], test_size=0.33, random_state=42, stratify=df['label'])\n",
        "best_model = gs.best_estimator_\n",
        "best_model.fit(X_train2, y_train2)\n",
        "preds = best_model.predict(X_test2)\n",
        "print(classification_report(y_test2, preds))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SavBmIdOkrGs",
      "metadata": {
        "id": "SavBmIdOkrGs"
      },
      "source": [
        "Try ngram_range=(1,2) or (1,3) for capturing phrases.\n",
        "\n",
        "Use stopword removal when vocabulary is noisy.\n",
        "\n",
        "Use character n-grams for short text (reviews, tweets).\n",
        "\n",
        "Limit max_features or use min_df to control vocabulary size.\n",
        "\n",
        "HashingVectorizer for memory-efficient transform on large corpora.\n",
        "\n",
        "Combine TF-IDF with pretrained sentence embeddings for hybrid features (concatenate dense + sparse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "a2bc90d8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['again' 'amazing' 'amazing and' 'amazing and works' 'and' 'and quick'\n",
            " 'and quick delivery' 'and works' 'and works great' 'broke' 'broke within'\n",
            " 'broke within days' 'buy' 'buy again' 'days' 'days worst'\n",
            " 'days worst purchase' 'delicious' 'delicious food' 'delicious food and'\n",
            " 'delivery' 'disappointed' 'enjoyed' 'enjoyed the'\n",
            " 'enjoyed the experience' 'ever' 'experience' 'experience very'\n",
            " 'experience very satisfied' 'extremely' 'extremely disappointed' 'food'\n",
            " 'food and' 'food and quick' 'great' 'is' 'is amazing' 'is amazing and'\n",
            " 'it' 'it is' 'it is amazing' 'love' 'love this' 'love this product'\n",
            " 'money' 'money extremely' 'money extremely disappointed' 'never'\n",
            " 'never buy' 'never buy again' 'not' 'not worth' 'not worth the' 'product'\n",
            " 'product broke' 'product broke within' 'product it' 'product it is'\n",
            " 'purchase' 'purchase ever' 'quick' 'quick delivery' 'really'\n",
            " 'really enjoyed' 'really enjoyed the' 'satisfied' 'service'\n",
            " 'service will' 'service will never' 'terrible' 'terrible service'\n",
            " 'terrible service will' 'the' 'the experience' 'the experience very'\n",
            " 'the money' 'the money extremely' 'the product' 'the product broke'\n",
            " 'this' 'this product' 'this product it' 'very' 'very satisfied' 'will'\n",
            " 'will never' 'will never buy' 'within' 'within days' 'within days worst'\n",
            " 'works' 'works great' 'worst' 'worst purchase' 'worst purchase ever'\n",
            " 'worth' 'worth the' 'worth the money']\n"
          ]
        }
      ],
      "source": [
        "#Try ngram_range=(1,2) or (1,3) for capturing phrases.\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(ngram_range=(1,3))\n",
        "X = tfidf.fit_transform(texts)\n",
        "print(tfidf.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "378b13aa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.46262479 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.46262479\n",
            "  0.46262479 0.         0.37935895 0.         0.         0.\n",
            "  0.         0.         0.         0.46262479 0.         0.        ]\n",
            " [0.         0.         0.57735027 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.57735027 0.57735027 0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.5        0.5        0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.5\n",
            "  0.5        0.         0.         0.         0.         0.        ]\n",
            " [0.         0.46262479 0.         0.46262479 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.37935895 0.46262479 0.         0.\n",
            "  0.         0.         0.         0.         0.46262479 0.        ]\n",
            " [0.         0.         0.         0.         0.5        0.5\n",
            "  0.         0.         0.         0.         0.5        0.\n",
            "  0.         0.         0.         0.         0.5        0.\n",
            "  0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.5        0.         0.         0.5        0.         0.\n",
            "  0.         0.5        0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.5       ]]\n"
          ]
        }
      ],
      "source": [
        "#Use stopword removal when vocabulary is noisy.\n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "X = tfidf.fit_transform(texts)\n",
        "print(X.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "7492353b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[' ag' ' aga' ' agai' ' am' ' ama' ' amaz' ' an' ' and' ' and ' ' br'\n",
            " ' bro' ' brok' ' bu' ' buy' ' buy ' ' da' ' day' ' days' ' de' ' del'\n",
            " ' deli' ' di' ' dis' ' disa' ' en' ' enj' ' enjo' ' ev' ' eve' ' ever'\n",
            " ' ex' ' exp' ' expe' ' ext' ' extr' ' fo' ' foo' ' food' ' gr' ' gre'\n",
            " ' grea' ' i ' ' i w' ' i wi' ' is' ' is ' ' is a' ' it' ' it ' ' it i'\n",
            " ' lo' ' lov' ' love' ' mo' ' mon' ' mone' ' ne' ' nev' ' neve' ' pr'\n",
            " ' pro' ' prod' ' pu' ' pur' ' purc' ' qu' ' qui' ' quic' ' sa' ' sat'\n",
            " ' sati' ' se' ' ser' ' serv' ' th' ' the' ' the ' ' thi' ' this' ' ve'\n",
            " ' ver' ' very' ' wi' ' wil' ' will' ' wit' ' with' ' wo' ' wor' ' work'\n",
            " ' wors' ' wort' ', e' ', ex' ', ext' ', i' ', i ' ', i w' ', it' ', it '\n",
            " ', v' ', ve' ', ver' ', w' ', wo' ', wor' 'aga' 'agai' 'again' 'ain'\n",
            " 'all' 'ally' 'ally ' 'ama' 'amaz' 'amazi' 'and' 'and ' 'and q' 'and w'\n",
            " 'app' 'appo' 'appoi' 'ase' 'ase ' 'ase e' 'ati' 'atis' 'atisf' 'ays'\n",
            " 'ays,' 'ays, ' 'azi' 'azin' 'azing' 'ble' 'ble ' 'ble s' 'bro' 'brok'\n",
            " 'broke' 'buy' 'buy ' 'buy a' 'ce,' 'ce, ' 'ce, i' 'ce, v' 'cha' 'chas'\n",
            " 'chase' 'cio' 'ciou' 'cious' 'ck ' 'ck d' 'ck de' 'ct ' 'ct b' 'ct br'\n",
            " 'ct,' 'ct, ' 'ct, i' 'd a' 'd an' 'd and' 'd q' 'd qu' 'd qui' 'd t'\n",
            " 'd th' 'd the' 'd w' 'd wo' 'd wor' 'day' 'days' 'days,' 'del' 'deli'\n",
            " 'delic' 'deliv' 'dis' 'disa' 'disap' 'duc' 'duct' 'duct ' 'duct,' 'e e'\n",
            " 'e ev' 'e eve' 'e ex' 'e exp' 'e m' 'e mo' 'e mon' 'e p' 'e pr' 'e pro'\n",
            " 'e s' 'e se' 'e ser' 'e t' 'e th' 'e thi' 'e w' 'e wi' 'e wit' 'e, '\n",
            " 'e, i' 'e, i ' 'e, v' 'e, ve' 'eal' 'eall' 'eally' 'eat' 'ed ' 'ed t'\n",
            " 'ed th' 'eli' 'elic' 'elici' 'eliv' 'elive' 'ely' 'ely ' 'ely d' 'eme'\n",
            " 'emel' 'emely' 'enc' 'ence' 'ence,' 'enj' 'enjo' 'enjoy' 'er ' 'er b'\n",
            " 'er bu' 'eri' 'erie' 'erien' 'err' 'erri' 'errib' 'erv' 'ervi' 'ervic'\n",
            " 'ery' 'ery ' 'ery s' 'eve' 'ever' 'ever ' 'exp' 'expe' 'exper' 'ext'\n",
            " 'extr' 'extre' 'ey,' 'ey, ' 'ey, e' 'fie' 'fied' 'foo' 'food' 'food '\n",
            " 'g a' 'g an' 'g and' 'gai' 'gain' 'gre' 'grea' 'great' 'h t' 'h th'\n",
            " 'h the' 'has' 'hase' 'hase ' 'he ' 'he e' 'he ex' 'he m' 'he mo' 'he p'\n",
            " 'he pr' 'hin' 'hin ' 'hin d' 'his' 'his ' 'his p' 'i l' 'i lo' 'i lov'\n",
            " 'i w' 'i wi' 'i wil' 'ibl' 'ible' 'ible ' 'ice' 'ice,' 'ice, ' 'ici'\n",
            " 'icio' 'iciou' 'ick' 'ick ' 'ick d' 'ied' 'ien' 'ienc' 'ience' 'ill'\n",
            " 'ill ' 'ill n' 'in ' 'in d' 'in da' 'ing' 'ing ' 'ing a' 'int' 'inte'\n",
            " 'inted' 'iou' 'ious' 'ious ' 'is ' 'is a' 'is am' 'is p' 'is pr' 'isa'\n",
            " 'isap' 'isapp' 'isf' 'isfi' 'isfie' 'it ' 'it i' 'it is' 'ith' 'ithi'\n",
            " 'ithin' 'ive' 'iver' 'ivery' 'joy' 'joye' 'joyed' 'k d' 'k de' 'k del'\n",
            " 'ke ' 'ke w' 'ke wi' 'ks ' 'ks g' 'ks gr' 'l n' 'l ne' 'l nev' 'le '\n",
            " 'le s' 'le se' 'lic' 'lici' 'licio' 'liv' 'live' 'liver' 'll ' 'll n'\n",
            " 'll ne' 'lly' 'lly ' 'lly e' 'lov' 'love' 'love ' 'ly ' 'ly d' 'ly di'\n",
            " 'ly e' 'ly en' 'maz' 'mazi' 'mazin' 'mel' 'mely' 'mely ' 'mon' 'mone'\n",
            " 'money' 'n d' 'n da' 'n day' 'nce' 'nce,' 'nce, ' 'nd ' 'nd q' 'nd qu'\n",
            " 'nd w' 'nd wo' 'nev' 'neve' 'never' 'ney' 'ney,' 'ney, ' 'ng ' 'ng a'\n",
            " 'ng an' 'njo' 'njoy' 'njoye' 'not' 'not ' 'not w' 'nte' 'nted' 'od '\n",
            " 'od a' 'od an' 'odu' 'oduc' 'oduct' 'oin' 'oint' 'ointe' 'oke' 'oke '\n",
            " 'oke w' 'one' 'oney' 'oney,' 'ood' 'ood ' 'ood a' 'ork' 'orks' 'orks '\n",
            " 'ors' 'orst' 'orst ' 'ort' 'orth' 'orth ' 'ot ' 'ot w' 'ot wo' 'ous'\n",
            " 'ous ' 'ous f' 'ove' 'ove ' 'ove t' 'oye' 'oyed' 'oyed ' 'per' 'peri'\n",
            " 'perie' 'poi' 'poin' 'point' 'ppo' 'ppoi' 'ppoin' 'pro' 'prod' 'produ'\n",
            " 'pur' 'purc' 'purch' 'qui' 'quic' 'quick' 'r b' 'r bu' 'r buy' 'rch'\n",
            " 'rcha' 'rchas' 'rea' 'real' 'reall' 'reat' 'rem' 'reme' 'remel' 'rib'\n",
            " 'ribl' 'rible' 'rie' 'rien' 'rienc' 'rks' 'rks ' 'rks g' 'rod' 'rodu'\n",
            " 'roduc' 'rok' 'roke' 'roke ' 'rri' 'rrib' 'rribl' 'rst' 'rst ' 'rst p'\n",
            " 'rth' 'rth ' 'rth t' 'rvi' 'rvic' 'rvice' 'ry ' 'ry s' 'ry sa' 's a'\n",
            " 's am' 's ama' 's f' 's fo' 's foo' 's g' 's gr' 's gre' 's p' 's pr'\n",
            " 's pro' 's, ' 's, w' 's, wo' 'sap' 'sapp' 'sappo' 'sat' 'sati' 'satis'\n",
            " 'se ' 'se e' 'se ev' 'ser' 'serv' 'servi' 'sfi' 'sfie' 'sfied' 'st '\n",
            " 'st p' 'st pu' 't b' 't br' 't bro' 't i' 't is' 't is ' 't p' 't pu'\n",
            " 't pur' 't w' 't wo' 't wor' 't, ' 't, i' 't, it' 'ted' 'ter' 'terr'\n",
            " 'terri' 'th ' 'th t' 'th th' 'the' 'the ' 'the e' 'the m' 'the p' 'thi'\n",
            " 'thin' 'thin ' 'this' 'this ' 'tis' 'tisf' 'tisfi' 'tre' 'trem' 'treme'\n",
            " 'uct' 'uct ' 'uct b' 'uct,' 'uct, ' 'uic' 'uick' 'uick ' 'urc' 'urch'\n",
            " 'urcha' 'us ' 'us f' 'us fo' 'uy ' 'uy a' 'uy ag' 've ' 've t' 've th'\n",
            " 'ver' 'ver ' 'ver b' 'very' 'very ' 'vic' 'vice' 'vice,' 'wil' 'will'\n",
            " 'will ' 'wit' 'with' 'withi' 'wor' 'work' 'works' 'wors' 'worst' 'wort'\n",
            " 'worth' 'xpe' 'xper' 'xperi' 'xtr' 'xtre' 'xtrem' 'y a' 'y ag' 'y aga'\n",
            " 'y d' 'y di' 'y dis' 'y e' 'y en' 'y enj' 'y s' 'y sa' 'y sat' 'y, '\n",
            " 'y, e' 'y, ex' 'yed' 'yed ' 'yed t' 'ys,' 'ys, ' 'ys, w' 'zin' 'zing'\n",
            " 'zing ']\n"
          ]
        }
      ],
      "source": [
        "#Use character n-grams for short text (reviews, tweets).\n",
        "tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3,5))\n",
        "X = tfidf.fit_transform(texts)\n",
        "print(tfidf.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "700895eb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(6, 1)\n",
            "['the']\n"
          ]
        }
      ],
      "source": [
        "#Limit max_features or use min_df to control vocabulary size.\n",
        "tfidf = TfidfVectorizer(max_features=5000, min_df=3)\n",
        "X = tfidf.fit_transform(texts)\n",
        "print(X.shape)\n",
        "print(tfidf.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "7ea593a4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(6, 4096)\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "#HashingVectorizer for memory-efficient transform on large corpora.\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "hv = HashingVectorizer(n_features=2**12)\n",
        "X = hv.transform(texts)\n",
        "print(X.shape)\n",
        "print(X.toarray())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "1f2d33e7",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\anaconda3\\envs\\tf\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\User\\anaconda3\\envs\\tf\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n",
            "(6, 420)\n"
          ]
        }
      ],
      "source": [
        "#Combine TF-IDF with pretrained sentence embeddings for hybrid features (concatenate dense + sparse)\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# TF-IDF\n",
        "tfidf = TfidfVectorizer()\n",
        "X_tfidf = tfidf.fit_transform(texts).toarray()\n",
        "\n",
        "# Embeddings\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "X_embed = model.encode(texts)\n",
        "\n",
        "# Combine\n",
        "X_final = np.hstack([X_tfidf, X_embed])\n",
        "print(X_final.shape)   # shows final features shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "IYb7iZ9VkmvQ",
      "metadata": {
        "id": "IYb7iZ9VkmvQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in c:\\users\\user\\anaconda3\\envs\\tf\\lib\\site-packages (5.1.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\user\\anaconda3\\envs\\tf\\lib\\site-packages (from sentence-transformers) (4.57.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\envs\\tf\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in c:\\users\\user\\anaconda3\\envs\\tf\\lib\\site-packages (from sentence-transformers) (2.9.1)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\user\\anaconda3\\envs\\tf\\lib\\site-packages (from sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in c:\\users\\user\\anaconda3\\envs\\tf\\lib\\site-packages (from sentence-transformers) (1.16.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\user\\anaconda3\\envs\\tf\\lib\\site-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in c:\\users\\user\\anaconda3\\envs\\tf\\lib\\site-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\user\\anaconda3\\envs\\tf\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\user\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\user\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\anaconda3\\envs\\tf\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.12.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\user\\anaconda3\\envs\\tf\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\user\\anaconda3\\envs\\tf\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\envs\\tf\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\anaconda3\\envs\\tf\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\tf\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\envs\\tf\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\anaconda3\\envs\\tf\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\envs\\tf\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\tf\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\tf\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.10.5)\n",
            "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\user\\anaconda3\\envs\\tf\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\anaconda3\\envs\\tf\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches: 100%|| 1/1 [00:00<00:00, 27.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embeddings shape: (6, 384)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "s_model = SentenceTransformer('all-MiniLM-L6-v2')  # example model\n",
        "sentence_embeddings = s_model.encode(df['text'].tolist(), show_progress_bar=True)\n",
        "print(\"Embeddings shape:\", sentence_embeddings.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "FeySzGt0kz4O",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeySzGt0kz4O",
        "outputId": "1afb0665-f2d7-4d98-9f4b-6eeb1e807b0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction: [1]\n"
          ]
        }
      ],
      "source": [
        "# =====  Save & load pipeline =====\n",
        "# Example: save best_model from GridSearch\n",
        "joblib.dump(best_model, \"best_text_pipeline.joblib\")\n",
        "# Load\n",
        "loaded = joblib.load(\"best_text_pipeline.joblib\")\n",
        "# Inference\n",
        "sample = [\"I really hate the support and the product quality\"]\n",
        "print(\"Prediction:\", loaded.predict(sample))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_FWK1DmKk9qQ",
      "metadata": {
        "id": "_FWK1DmKk9qQ"
      },
      "source": [
        "Inference pipeline (production notes) (cell: markdown)\n",
        "\n",
        "Pipeline should include preprocessing  vectorization  model. Save it as a single Pipeline object (as above).\n",
        "\n",
        "Ensure same text cleaning/normalization used at train time is applied during inference.\n",
        "\n",
        "For scaling to production: wrap pipeline in a small API (FastAPI/Flask) and serve with Gunicorn/Uvicorn + container (Docker).\n",
        "\n",
        "If model needs to be retrained periodically, automate dataset collection, validation, and CI tests."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FwAMfd0plQR6",
      "metadata": {
        "id": "FwAMfd0plQR6"
      },
      "source": [
        "<h2>Short assignment / exercises</h2>\n",
        "<h4>Task - 1 : Compare performances: CountVectorizer vs TfidfVectorizer vs Word2Vec averaged embeddings on a 10k-sample dataset.</h4>\n",
        "\n",
        "<h4>Task - 2 : Try ngram_range=(1,3) and observe overfitting/feature explosion.</h4>\n",
        "\n",
        "<h4>Task - 3 : Use GridSearchCV to tune C for Logistic Regression and alpha for MultinomialNB.</h4>\n",
        "\n",
        "<h4>Task - 4 : Create an inference API using FastAPI that loads best_text_pipeline.joblib and exposes POST /predict.</h4>\n",
        "\n",
        "<h4>(Advanced) Fine-tune a small transformer (e.g., DistilBERT) for sentiment classification using Hugging Face transformers</h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a019a3a0",
      "metadata": {},
      "source": [
        "<h3>Task - 1</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7183e049",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "lcCP1CX4lEnL",
      "metadata": {
        "id": "lcCP1CX4lEnL"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>text</th>\n",
              "      <th>selected_text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>Time of Tweet</th>\n",
              "      <th>Age of User</th>\n",
              "      <th>Country</th>\n",
              "      <th>Population -2020</th>\n",
              "      <th>Land Area (Km)</th>\n",
              "      <th>Density (P/Km)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cb774db0d1</td>\n",
              "      <td>I`d have responded, if I were going</td>\n",
              "      <td>I`d have responded, if I were going</td>\n",
              "      <td>neutral</td>\n",
              "      <td>morning</td>\n",
              "      <td>0-20</td>\n",
              "      <td>Afghanistan</td>\n",
              "      <td>38928346</td>\n",
              "      <td>652860.0</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>549e992a42</td>\n",
              "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
              "      <td>Sooo SAD</td>\n",
              "      <td>negative</td>\n",
              "      <td>noon</td>\n",
              "      <td>21-30</td>\n",
              "      <td>Albania</td>\n",
              "      <td>2877797</td>\n",
              "      <td>27400.0</td>\n",
              "      <td>105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>088c60f138</td>\n",
              "      <td>my boss is bullying me...</td>\n",
              "      <td>bullying me</td>\n",
              "      <td>negative</td>\n",
              "      <td>night</td>\n",
              "      <td>31-45</td>\n",
              "      <td>Algeria</td>\n",
              "      <td>43851044</td>\n",
              "      <td>2381740.0</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       textID                                            text  \\\n",
              "0  cb774db0d1             I`d have responded, if I were going   \n",
              "1  549e992a42   Sooo SAD I will miss you here in San Diego!!!   \n",
              "2  088c60f138                       my boss is bullying me...   \n",
              "\n",
              "                         selected_text sentiment Time of Tweet Age of User  \\\n",
              "0  I`d have responded, if I were going   neutral       morning        0-20   \n",
              "1                             Sooo SAD  negative          noon       21-30   \n",
              "2                          bullying me  negative         night       31-45   \n",
              "\n",
              "       Country  Population -2020  Land Area (Km)  Density (P/Km)  \n",
              "0  Afghanistan          38928346         652860.0               60  \n",
              "1      Albania           2877797          27400.0              105  \n",
              "2      Algeria          43851044        2381740.0               18  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train = pd.read_csv(r\"C:\\Users\\User\\OneDrive\\Desktop\\infosys_springboard_project\\Assignment2\\sentiment_analysis_dataset\\train.csv\",encoding=\"latin1\")\n",
        "test  = pd.read_csv(r\"C:\\Users\\User\\OneDrive\\Desktop\\infosys_springboard_project\\Assignment2\\sentiment_analysis_dataset\\train.csv\",encoding=\"latin1\")\n",
        "\n",
        "train.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9cacd33e",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train = train[\"text\"].astype(str).tolist()\n",
        "y_train = train[\"sentiment\"].tolist()\n",
        "\n",
        "X_test  = test[\"text\"].astype(str).tolist()\n",
        "y_test  = test[\"sentiment\"].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8dc5c8cf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CountVectorizer acc: 0.8585568210763801\n"
          ]
        }
      ],
      "source": [
        "cv = CountVectorizer(max_features=10000)\n",
        "Xtr_cv = cv.fit_transform(X_train)\n",
        "Xte_cv = cv.transform(X_test)\n",
        "\n",
        "lr_cv = LogisticRegression(max_iter=1000)\n",
        "lr_cv.fit(Xtr_cv, y_train)\n",
        "pred_cv = lr_cv.predict(Xte_cv)\n",
        "\n",
        "print(\"CountVectorizer acc:\", accuracy_score(y_test, pred_cv))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "51100760",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF acc: 0.7932025763254612\n"
          ]
        }
      ],
      "source": [
        "tfidf = TfidfVectorizer(max_features=10000)\n",
        "Xtr_tf = tfidf.fit_transform(X_train)\n",
        "Xte_tf = tfidf.transform(X_test)\n",
        "\n",
        "lr_tf = LogisticRegression(max_iter=1000)\n",
        "lr_tf.fit(Xtr_tf, y_train)\n",
        "pred_tf = lr_tf.predict(Xte_tf)\n",
        "\n",
        "print(\"TF-IDF acc:\", accuracy_score(y_test, pred_tf))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f2ba93c5",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2Vec avg acc: 0.5634802226993195\n"
          ]
        }
      ],
      "source": [
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "train_tokens = [simple_preprocess(t) for t in X_train]\n",
        "test_tokens  = [simple_preprocess(t) for t in X_test]\n",
        "\n",
        "w2v = Word2Vec(sentences=train_tokens, vector_size=100, window=5, min_count=2, workers=4)\n",
        "\n",
        "def sent2vec(tokens):\n",
        "    vecs = [w2v.wv[w] for w in tokens if w in w2v.wv]\n",
        "    if not vecs:\n",
        "        return np.zeros(w2v.vector_size)\n",
        "    return np.mean(vecs, axis=0)\n",
        "\n",
        "Xtr_w2v = np.vstack([sent2vec(toks) for toks in train_tokens])\n",
        "Xte_w2v = np.vstack([sent2vec(toks) for toks in test_tokens])\n",
        "\n",
        "lr_w2v = LogisticRegression(max_iter=1000)\n",
        "lr_w2v.fit(Xtr_w2v, y_train)\n",
        "pred_w2v = lr_w2v.predict(Xte_w2v)\n",
        "\n",
        "print(\"Word2Vec avg acc:\", accuracy_score(y_test, pred_w2v))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "35bc3f1d",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc_cv   = accuracy_score(y_test, pred_cv)    # CountVectorizer\n",
        "acc_tf   = accuracy_score(y_test, pred_tf)    # TF-IDF\n",
        "acc_w2v  = accuracy_score(y_test, pred_w2v)   # Word2Vec\n",
        "\n",
        "# bar chart\n",
        "models = [\"CountVectorizer\", \"TF-IDF\", \"Word2Vec\"]\n",
        "scores = [acc_cv, acc_tf, acc_w2v]\n",
        "\n",
        "plt.figure()\n",
        "plt.bar(models, scores)\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy Comparison: Count vs TF-IDF vs Word2Vec\")\n",
        "plt.ylim(0, 1)   # because accuracy is between 0 and 1\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd3707cd",
      "metadata": {},
      "source": [
        "<h3>Task - 2</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "da02b754",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature size (1,3): (27481, 416924)\n",
            "Train acc (1,3): 0.9394126851279065\n",
            "Test acc  (1,3): 0.9394126851279065\n"
          ]
        }
      ],
      "source": [
        "tfidf_13 = TfidfVectorizer(ngram_range=(1,3))\n",
        "Xtr_13 = tfidf_13.fit_transform(X_train)\n",
        "Xte_13 = tfidf_13.transform(X_test)\n",
        "\n",
        "print(\"Feature size (1,3):\", Xtr_13.shape)\n",
        "\n",
        "lr_13 = LogisticRegression(max_iter=1000)\n",
        "lr_13.fit(Xtr_13, y_train)\n",
        "\n",
        "pred_train_13 = lr_13.predict(Xtr_13)\n",
        "pred_test_13  = lr_13.predict(Xte_13)\n",
        "\n",
        "print(\"Train acc (1,3):\", accuracy_score(y_train, pred_train_13))\n",
        "print(\"Test acc  (1,3):\", accuracy_score(y_test,  pred_test_13))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "924afea4",
      "metadata": {},
      "source": [
        "<h3>Task - 3</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "3f96f7b8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best LR: {'clf__C': 1} CV score: 0.6874204768426119\n",
            "Test acc (best LR): 0.8151086205014374\n"
          ]
        }
      ],
      "source": [
        "#Logistic Regression + TF-IDF\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "lr_pipe = Pipeline([\n",
        "    (\"tfidf\", TfidfVectorizer()),\n",
        "    (\"clf\", LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "param_grid_lr = {\n",
        "    \"clf__C\": [0.01, 0.1, 1, 10]\n",
        "}\n",
        "\n",
        "gs_lr = GridSearchCV(\n",
        "    lr_pipe,\n",
        "    param_grid_lr,\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    scoring=\"accuracy\"\n",
        ")\n",
        "gs_lr.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best LR:\", gs_lr.best_params_, \"CV score:\", gs_lr.best_score_)\n",
        "best_lr = gs_lr.best_estimator_\n",
        "print(\"Test acc (best LR):\", accuracy_score(y_test, best_lr.predict(X_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "bcab600e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best NB: {'clf__alpha': 0.5} CV score: 0.6172264768041602\n",
            "Test acc (best NB): 0.8212947127106001\n"
          ]
        }
      ],
      "source": [
        "#MultinomialNB + TF-IDF\n",
        "nb_pipe = Pipeline([\n",
        "    (\"tfidf\", TfidfVectorizer()),\n",
        "    (\"clf\", MultinomialNB())\n",
        "])\n",
        "\n",
        "param_grid_nb = {\n",
        "    \"clf__alpha\": [0.1, 0.5, 1.0, 2.0]\n",
        "}\n",
        "\n",
        "gs_nb = GridSearchCV(\n",
        "    nb_pipe,\n",
        "    param_grid_nb,\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    scoring=\"accuracy\"\n",
        ")\n",
        "gs_nb.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best NB:\", gs_nb.best_params_, \"CV score:\", gs_nb.best_score_)\n",
        "best_nb = gs_nb.best_estimator_\n",
        "print(\"Test acc (best NB):\", accuracy_score(y_test, best_nb.predict(X_test)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27ce23c1",
      "metadata": {},
      "source": [
        "<h3>Task - 4</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "1d135054",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['best_text_pipeline.joblib']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "joblib.dump(best_lr, \"best_text_pipeline.joblib\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (tf)",
      "language": "python",
      "name": "tf"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
